{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                # Numpy nunca puede faltar\n",
    "import pandas as pd               # El siempre confiable Pandas\n",
    "from os import listdir, path      # Para leer/escribir archivos del sistema\n",
    "\n",
    "import librosa                    # Se usa para análisis de canciones pero hay una cosa que nos viene bien de aquí\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import statsmodels as sm\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa import stattools\n",
    "\n",
    "import librosa\n",
    "from scipy.signal import find_peaks, find_peaks_cwt, peak_prominences, periodogram, stft, peak_widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_df(df, whole_stats):\n",
    "    nrows, ncols = df.shape\n",
    "    nans_per_sensor = df.isna().sum(axis=0)\n",
    "    for sensor in df.columns:\n",
    "        n_nans = nans_per_sensor[sensor]\n",
    "        if n_nans:\n",
    "            mean = whole_stats.loc[sensor,\"mean\"]\n",
    "            if n_nans == nrows:\n",
    "                df[sensor].fillna(mean, inplace=True)\n",
    "    \n",
    "            else:\n",
    "                data = df[sensor].values\n",
    "                nan_mask = np.isnan(data)\n",
    "                not_nan_indexes = np.argwhere(~nan_mask).squeeze()\n",
    "                first_not_nan_index = not_nan_indexes[0]\n",
    "                last_not_nan_index = not_nan_indexes[-1]\n",
    "                nans_batch1 = nan_mask.copy()\n",
    "                nans_batch1[last_not_nan_index+1:] = False\n",
    "                data[nans_batch1] = mean\n",
    "                \n",
    "                # holt winters\n",
    "                if last_not_nan_index != nrows-1:\n",
    "                    #print(first_not_nan_index,last_not_nan_index)\n",
    "                    autocorrelation = stattools.acf(data[:last_not_nan_index], nlags=40, fft=False)\n",
    "                    higher_correlation = np.sort(autocorrelation)[-2]\n",
    "                    period = np.argwhere(autocorrelation == higher_correlation).squeeze() + 1\n",
    "                    holtwinters = ExponentialSmoothing(data[:last_not_nan_index], trend=None, seasonal=\"add\", \n",
    "                                                       seasonal_periods=period, \n",
    "                                                       initialization_method=\"estimated\").fit(optimized=True)\n",
    "                    #print(last_not_nan_index)\n",
    "                    forecasting = holtwinters.predict(start=last_not_nan_index+1, end=nrows-1)\n",
    "                    #print(forecasting.shape)\n",
    "                    #print(nan_mask.sum())\n",
    "                    data[last_not_nan_index+1:] = forecasting\n",
    "    \n",
    "                df[sensor] = data\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estandarizar(df):\n",
    "    aggs = df.agg([np.nanmean, np.nanstd]).astype(\"float16\")\n",
    "    standarized_df = (df - aggs.loc[\"nanmean\",:])/ aggs.loc[\"nanstd\",:]\n",
    "    return standarized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars(df, nombre):\n",
    "    cars = [int(nombre[:-4])]\n",
    "    # zeros_crossings\n",
    "    cars.extend(librosa.zero_crossings(df.values, axis = 0).sum(axis = 0))\n",
    "    \n",
    "    # find_peaks\n",
    "    cars.extend(df.apply(find_peaks, axis = 0).iloc[0,:].apply(len).values)\n",
    "            \n",
    "    # peak_widths_max\n",
    "    λ0 = lambda x: np.max(peak_widths(x, find_peaks(x)[0])[0]) if len(find_peaks(x)[0]) != 0 else 0\n",
    "    cars.extend(df.apply(λ0).values)\n",
    "                \n",
    "    # peak_widths_mean\n",
    "    λ01 = lambda x: np.mean(peak_widths(x, find_peaks(x)[0])[0]) if len(find_peaks(x)[0]) != 0 else 0\n",
    "    cars.extend(df.apply(λ01).values)\n",
    "                \n",
    "    # peak_prominences_max\n",
    "    λ1 = lambda x: np.max(peak_prominences(x, find_peaks(x)[0])[0]) if len(find_peaks(x)[0]) != 0 else 0\n",
    "    cars.extend(df.apply(λ1).values)\n",
    "                \n",
    "    # peak_prominences_mean\n",
    "    λ11 = lambda x: np.mean(peak_prominences(x, find_peaks(x)[0])[0]) if len(find_peaks(x)[0]) != 0 else 0\n",
    "    cars.extend(df.apply(λ11).values)\n",
    "                \n",
    "    # periodogram_max\n",
    "    λ2 = lambda x: np.max(periodogram(x, 100)[1]) if ~x.isna().all() else 0\n",
    "    cars.extend(np.sqrt(df.apply(λ2).values)) # Es un estimado del RMS\n",
    "    \n",
    "    # periodogram_mean\n",
    "    λ3 = lambda x: np.mean(periodogram(x, 100)[1]) if ~x.isna().all() else 0\n",
    "    cars.extend(df.apply(λ3).values)\n",
    "                \n",
    "    return cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_stats_path = \"csvs/whole_stats.csv\"\n",
    "whole_stats = pd.read_csv(whole_stats_path, inddex_col=0)\n",
    "whole_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_originales = \"\"\n",
    "path_copias = \"estandarizados\"\n",
    "for carpeta in [\"train\", \"test\"]:\n",
    "    camino = path.join(path_originales, carpeta)\n",
    "    print(f\"Comienzo con {carpeta}\")\n",
    "    for file in listdir(camino):\n",
    "        lectura = path.join(camino, file)\n",
    "        escritura = path.join(path_copias, carpeta, file)\n",
    "        df = pd.read_csv(lectura)\n",
    "        df = impute_df(df)\n",
    "        s_df = estandarizar(df).astype(\"Float16\")\n",
    "        s_df.to_csv(escritura, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path_train = \"estandarizados/train\"\n",
    "todos = []\n",
    "i = 0\n",
    "for archivo in listdir(path_train):\n",
    "    camino = path.join(path_train, archivo)\n",
    "    df = pd.read_csv(camino)\n",
    "    try:\n",
    "        chars = get_chars(df, archivo)\n",
    "        todos.append(chars)\n",
    "    except Exception as e:\n",
    "        print(e,archivo)\n",
    "    #print(i)\n",
    "    i += 1\n",
    "    \n",
    "caracteristicas = \"zcr peaks peak_wmax peak_wmean peak_promax peak_promean period_max period_mean\". split()\n",
    "cols = [\"segment_id\"] + [f\"sensor_{idx}_{car}\" for car in caracteristicas for idx in range(1,11)]\n",
    "df_todos = pd.DataFrame(todos, columns = cols)\n",
    "df_todos.to_csv(\"csvs/stats_per_file_signal_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
