{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "from threading import Thread\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "#        pass\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/oscar/Escritorio/predict-volcanic-eruptions-ingv-oe/train\" # Oscar\n",
    "train_filenames = os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60001.000000</td>\n",
       "      <td>60001.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>60001.000000</td>\n",
       "      <td>60001.000000</td>\n",
       "      <td>60001.000000</td>\n",
       "      <td>60001.00000</td>\n",
       "      <td>60001.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>60001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.593423</td>\n",
       "      <td>2.545708</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.661839</td>\n",
       "      <td>-0.458976</td>\n",
       "      <td>1.431776</td>\n",
       "      <td>1.21018</td>\n",
       "      <td>3.117231</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-7.668739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>387.653854</td>\n",
       "      <td>553.938047</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>319.048016</td>\n",
       "      <td>170.261498</td>\n",
       "      <td>1016.208891</td>\n",
       "      <td>396.16904</td>\n",
       "      <td>594.306992</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>669.995033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3448.000000</td>\n",
       "      <td>-7088.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-3628.000000</td>\n",
       "      <td>-1452.000000</td>\n",
       "      <td>-3690.000000</td>\n",
       "      <td>-2132.00000</td>\n",
       "      <td>-2444.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-4662.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-210.000000</td>\n",
       "      <td>-229.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-186.000000</td>\n",
       "      <td>-92.000000</td>\n",
       "      <td>-678.000000</td>\n",
       "      <td>-236.00000</td>\n",
       "      <td>-400.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-358.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>213.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>684.000000</td>\n",
       "      <td>243.00000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>356.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3478.000000</td>\n",
       "      <td>9804.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3141.000000</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>1843.00000</td>\n",
       "      <td>2243.000000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3951.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sensor_1      sensor_2 sensor_3      sensor_4      sensor_5  \\\n",
       "count  60001.000000  60001.000000        0  60001.000000  60001.000000   \n",
       "mean       7.593423      2.545708     <NA>      0.661839     -0.458976   \n",
       "std      387.653854    553.938047     <NA>    319.048016    170.261498   \n",
       "min    -3448.000000  -7088.000000     <NA>  -3628.000000  -1452.000000   \n",
       "25%     -210.000000   -229.000000     <NA>   -186.000000    -92.000000   \n",
       "50%        0.000000      0.000000     <NA>      0.000000      0.000000   \n",
       "75%      213.000000    232.000000     <NA>    181.000000     93.000000   \n",
       "max     3478.000000   9804.000000     <NA>   3141.000000   1457.000000   \n",
       "\n",
       "           sensor_6     sensor_7      sensor_8 sensor_9     sensor_10  \n",
       "count  60001.000000  60001.00000  60001.000000        0  60001.000000  \n",
       "mean       1.431776      1.21018      3.117231     <NA>     -7.668739  \n",
       "std     1016.208891    396.16904    594.306992     <NA>    669.995033  \n",
       "min    -3690.000000  -2132.00000  -2444.000000     <NA>  -4662.000000  \n",
       "25%     -678.000000   -236.00000   -400.000000     <NA>   -358.000000  \n",
       "50%        0.000000      0.00000      0.000000     <NA>      0.000000  \n",
       "75%      684.000000    243.00000    413.000000     <NA>    356.000000  \n",
       "max     4240.000000   1843.00000   2243.000000     <NA>   3951.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filename = train_filenames[0]\n",
    "data = pd.read_csv( os.path.join(train_dir,train_filename), dtype=\"Int16\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checando que tan buena estuvo la prank\n",
    "\n",
    "Según distintos archivos, hay ciertos que no tienen valores registrados (NaNs) o cosas raras como puros ceros. Sería importante ver como está dicha situación para todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prank(data):\n",
    "    \"\"\"Function to check if some columns contain only nans or some constant\"\"\"\n",
    "    nrows, _ = data.shape\n",
    "    only_nans = [data[sensor].isna().sum() for sensor in data.columns]\n",
    "    only_constants = [data[sensor].std() == 0 for sensor in data.columns]\n",
    "    \n",
    "    return only_nans, only_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_partition(data, npartitions):\n",
    "    n_elements = len(data)\n",
    "    n = n_elements//npartitions\n",
    "    \n",
    "    for k in range(npartitions):\n",
    "        i = k*n\n",
    "        j = (k+1)*n\n",
    "        if k != npartitions - 1:\n",
    "            yield data[i:j]\n",
    "        else:\n",
    "            yield data[i:]  # Last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(nthreads):\n",
    "    def decorator(original_function):\n",
    "        def parallelized_function(dir_path, filenames, verbose=True):\n",
    "            parallel_ans = []\n",
    "            threads = []\n",
    "            counter = 1\n",
    "    \n",
    "            # Partir los datos\n",
    "            for partition in yield_partition(filenames, nthreads):\n",
    "                thread = Thread(target=original_function, args=(dir_path, partition, verbose, parallel_ans))\n",
    "                threads.append(thread)\n",
    "                print(\"Starting\",counter)\n",
    "                thread.start()\n",
    "                counter += 1 \n",
    "            \n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "            \n",
    "            list_nans = [df_nan for df_nan, df_constant in parallel_ans]\n",
    "            list_constants = [df_constant for df_nan, df_constant in parallel_ans]\n",
    "            concatenation_nans = pd.concat(list_nans)\n",
    "            concatenation_constants = pd.concat(list_constants)\n",
    "            return concatenation_nans, concatenation_constants\n",
    "        \n",
    "        return parallelized_function\n",
    "    \n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@parallelize(8)\n",
    "def check_pranks(dir_path, filenames, verbose=True, parallel_ans = None):\n",
    "    nans = []\n",
    "    constants = []\n",
    "    for i,filename in enumerate(filenames,1):\n",
    "        df = pd.read_csv(os.path.join(dir_path,filename), dtype=\"Int16\")\n",
    "        only_nans, only_constants = check_prank(df)\n",
    "        nans.append(only_nans)\n",
    "        constants.append(only_constants)\n",
    "        if verbose and i%10 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    df_nans = pd.DataFrame(nans, columns=df.columns)\n",
    "    df_constants = pd.DataFrame(constants, columns=df.columns)\n",
    "    \n",
    "    if parallel_ans != None:\n",
    "        parallel_ans.append( (df_nans, df_constants) )\n",
    "    else:\n",
    "        return df_nans, df_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train_nans, df_train_constants = check_pranks(train_dir, train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_nans.to_csv(\"df_train_nans.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_nans.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_constants.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_constants.to_csv(\"df_train_constants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/home/oscar/Escritorio/predict-volcanic-eruptions-ingv-oe/test\"\n",
    "test_filenames = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Starting 4\n",
      "Starting 5\n",
      "Starting 6\n",
      "Starting 7\n",
      "Starting 8\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "20\n",
      "20\n",
      "20\n",
      "2020\n",
      "\n",
      "20\n",
      "20\n",
      "20\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "90\n",
      "90\n",
      "90\n",
      "9090\n",
      "\n",
      "90\n",
      "90\n",
      "90\n",
      "100\n",
      "100\n",
      "100\n",
      "100100\n",
      "\n",
      "100\n",
      "100\n",
      "100\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120120\n",
      "\n",
      "120\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "140\n",
      "140\n",
      "140\n",
      "140\n",
      "140140\n",
      "\n",
      "140\n",
      "140\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "160160\n",
      "\n",
      "160\n",
      "160\n",
      "160\n",
      "160\n",
      "160\n",
      "160\n",
      "170\n",
      "170170\n",
      "\n",
      "170170\n",
      "170\n",
      "\n",
      "170\n",
      "170\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "180180\n",
      "\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "200200\n",
      "\n",
      "200\n",
      "200200\n",
      "\n",
      "200\n",
      "200200\n",
      "\n",
      "210\n",
      "210\n",
      "210\n",
      "210\n",
      "210210\n",
      "\n",
      "210\n",
      "210\n",
      "220\n",
      "220\n",
      "220\n",
      "220\n",
      "220220\n",
      "\n",
      "220\n",
      "220\n",
      "230\n",
      "230\n",
      "230\n",
      "230\n",
      "230\n",
      "230230\n",
      "\n",
      "230\n",
      "240\n",
      "240\n",
      "240\n",
      "240\n",
      "240240\n",
      "\n",
      "240\n",
      "240\n",
      "250250250\n",
      "\n",
      "\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "260\n",
      "260\n",
      "260\n",
      "260260\n",
      "\n",
      "260260\n",
      "\n",
      "260\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "280\n",
      "280\n",
      "280\n",
      "280\n",
      "280\n",
      "280\n",
      "280280\n",
      "\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "320\n",
      "320320\n",
      "\n",
      "320\n",
      "320\n",
      "320\n",
      "320\n",
      "320\n",
      "330\n",
      "330\n",
      "330\n",
      "330\n",
      "330330\n",
      "\n",
      "330\n",
      "330\n",
      "340\n",
      "340\n",
      "340\n",
      "340\n",
      "340340340\n",
      "\n",
      "\n",
      "340\n",
      "350\n",
      "350350\n",
      "\n",
      "350350\n",
      "\n",
      "350350\n",
      "\n",
      "350\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "380\n",
      "380\n",
      "380380\n",
      "\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "410410\n",
      "\n",
      "410\n",
      "410\n",
      "410\n",
      "410\n",
      "410\n",
      "410\n",
      "420\n",
      "420\n",
      "420\n",
      "420\n",
      "420\n",
      "420420\n",
      "420\n",
      "\n",
      "430\n",
      "430430\n",
      "\n",
      "430\n",
      "430\n",
      "430\n",
      "430\n",
      "430\n",
      "440\n",
      "440\n",
      "440440440\n",
      "\n",
      "\n",
      "440\n",
      "440440\n",
      "\n",
      "450\n",
      "450450\n",
      "\n",
      "450\n",
      "450\n",
      "450\n",
      "450450\n",
      "\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500500\n",
      "\n",
      "500\n",
      "510\n",
      "510\n",
      "510\n",
      "510510\n",
      "\n",
      "510\n",
      "510\n",
      "510\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "540\n",
      "540\n",
      "540540\n",
      "\n",
      "540\n",
      "540\n",
      "540540\n",
      "\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "560\n",
      "560\n",
      "560\n",
      "560560\n",
      "\n",
      "560\n",
      "560\n",
      "560\n",
      "CPU times: user 13min 47s, sys: 49.6 s, total: 14min 37s\n",
      "Wall time: 12min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test_nans, df_test_constants = check_pranks(test_dir, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nans.to_csv(\"df_test_nans.csv\", index=False)\n",
    "df_test_constants.to_csv(\"df_test_constants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor_1     23328531\n",
       "sensor_2     75807628\n",
       "sensor_3     26593834\n",
       "sensor_4        13406\n",
       "sensor_5     92651510\n",
       "sensor_6        57658\n",
       "sensor_7      2713162\n",
       "sensor_8     17653700\n",
       "sensor_9     51208744\n",
       "sensor_10    46050548\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_nans.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor_1       0\n",
       "sensor_2       0\n",
       "sensor_3       0\n",
       "sensor_4       0\n",
       "sensor_5       0\n",
       "sensor_6       0\n",
       "sensor_7       0\n",
       "sensor_8       0\n",
       "sensor_9       0\n",
       "sensor_10    815\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_constants.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# Zona después del \\end{document}\n",
    "\n",
    "`time_to_eruption` es el número de muestras antes de la siguiente erupción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estandarizando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path_originales = \"/home/oscar/Escritorio/predict-volcanic-eruptions-ingv-oe\"\n",
    "path_copias = \"/home/oscar/Escritorio/estandarizados\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def estandarizar(df):\n",
    "    aggs = df.agg([np.nanmean, np.nanstd]).astype(\"Float16\")\n",
    "    standarized_df = (df - aggs.loc[\"nanmean\",:])/ aggs.loc[\"nanstd\",:]\n",
    "    return standarized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           sensor_1    sensor_2    sensor_3    sensor_4    sensor_5  \\\n",
      "nanmean   -1.207031   -0.921875    1.230469    1.071289    0.858887   \n",
      "nanstd   401.000000  444.000000  452.250000  388.500000  206.250000   \n",
      "\n",
      "           sensor_6    sensor_7   sensor_8    sensor_9   sensor_10  \n",
      "nanmean   11.429688    6.132812   -0.67334    0.802246   -8.828125  \n",
      "nanstd   760.000000  870.000000  810.50000  337.750000  783.500000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.127698</td>\n",
       "      <td>0.911986</td>\n",
       "      <td>0.211762</td>\n",
       "      <td>-2.218974</td>\n",
       "      <td>1.600684</td>\n",
       "      <td>-0.126881</td>\n",
       "      <td>0.455020</td>\n",
       "      <td>0.047715</td>\n",
       "      <td>-0.778097</td>\n",
       "      <td>0.974892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.933185</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.320110</td>\n",
       "      <td>-2.211252</td>\n",
       "      <td>1.736442</td>\n",
       "      <td>0.127066</td>\n",
       "      <td>0.181457</td>\n",
       "      <td>0.283372</td>\n",
       "      <td>-0.502745</td>\n",
       "      <td>0.858747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.796028</td>\n",
       "      <td>0.821896</td>\n",
       "      <td>0.227241</td>\n",
       "      <td>-2.103144</td>\n",
       "      <td>1.498866</td>\n",
       "      <td>0.256014</td>\n",
       "      <td>-0.199003</td>\n",
       "      <td>0.332725</td>\n",
       "      <td>-0.162257</td>\n",
       "      <td>0.761746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.678821</td>\n",
       "      <td>0.504329</td>\n",
       "      <td>0.110049</td>\n",
       "      <td>-2.077404</td>\n",
       "      <td>0.999472</td>\n",
       "      <td>0.286277</td>\n",
       "      <td>-0.435785</td>\n",
       "      <td>0.399350</td>\n",
       "      <td>-0.123767</td>\n",
       "      <td>0.639219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.469344</td>\n",
       "      <td>0.080905</td>\n",
       "      <td>0.160906</td>\n",
       "      <td>-2.105718</td>\n",
       "      <td>0.432199</td>\n",
       "      <td>0.382329</td>\n",
       "      <td>-0.292107</td>\n",
       "      <td>0.289541</td>\n",
       "      <td>-0.366550</td>\n",
       "      <td>0.470744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>-0.281279</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>-0.433898</td>\n",
       "      <td>1.225042</td>\n",
       "      <td>-0.299922</td>\n",
       "      <td>-0.462407</td>\n",
       "      <td>-1.181762</td>\n",
       "      <td>-0.112679</td>\n",
       "      <td>0.270016</td>\n",
       "      <td>-0.340998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>-0.231404</td>\n",
       "      <td>-0.189365</td>\n",
       "      <td>-0.139813</td>\n",
       "      <td>1.338298</td>\n",
       "      <td>-0.207801</td>\n",
       "      <td>-0.257144</td>\n",
       "      <td>-0.763371</td>\n",
       "      <td>-0.071964</td>\n",
       "      <td>0.329231</td>\n",
       "      <td>-0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>-0.159085</td>\n",
       "      <td>-0.297473</td>\n",
       "      <td>0.355488</td>\n",
       "      <td>1.423240</td>\n",
       "      <td>-0.154467</td>\n",
       "      <td>-0.349250</td>\n",
       "      <td>-0.476015</td>\n",
       "      <td>-0.071964</td>\n",
       "      <td>0.181192</td>\n",
       "      <td>-0.131681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>-0.034396</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>0.534593</td>\n",
       "      <td>1.418092</td>\n",
       "      <td>-0.159316</td>\n",
       "      <td>-0.459776</td>\n",
       "      <td>-0.199003</td>\n",
       "      <td>-0.048521</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>-0.016812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>0.012985</td>\n",
       "      <td>-0.365041</td>\n",
       "      <td>0.749076</td>\n",
       "      <td>1.539070</td>\n",
       "      <td>-0.290225</td>\n",
       "      <td>-0.096618</td>\n",
       "      <td>-0.104750</td>\n",
       "      <td>-0.365610</td>\n",
       "      <td>0.042036</td>\n",
       "      <td>0.036794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60001 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n",
       "0      2.127698  0.911986  0.211762 -2.218974  1.600684 -0.126881  0.455020   \n",
       "1      1.933185  0.972797  0.320110 -2.211252  1.736442  0.127066  0.181457   \n",
       "2      1.796028  0.821896  0.227241 -2.103144  1.498866  0.256014 -0.199003   \n",
       "3      1.678821  0.504329  0.110049 -2.077404  0.999472  0.286277 -0.435785   \n",
       "4      1.469344  0.080905  0.160906 -2.105718  0.432199  0.382329 -0.292107   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "59996 -0.281279  0.004329 -0.433898  1.225042 -0.299922 -0.462407 -1.181762   \n",
       "59997 -0.231404 -0.189365 -0.139813  1.338298 -0.207801 -0.257144 -0.763371   \n",
       "59998 -0.159085 -0.297473  0.355488  1.423240 -0.154467 -0.349250 -0.476015   \n",
       "59999 -0.034396 -0.365041  0.534593  1.418092 -0.159316 -0.459776 -0.199003   \n",
       "60000  0.012985 -0.365041  0.749076  1.539070 -0.290225 -0.096618 -0.104750   \n",
       "\n",
       "       sensor_8  sensor_9  sensor_10  \n",
       "0      0.047715 -0.778097   0.974892  \n",
       "1      0.283372 -0.502745   0.858747  \n",
       "2      0.332725 -0.162257   0.761746  \n",
       "3      0.399350 -0.123767   0.639219  \n",
       "4      0.289541 -0.366550   0.470744  \n",
       "...         ...       ...        ...  \n",
       "59996 -0.112679  0.270016  -0.340998  \n",
       "59997 -0.071964  0.329231  -0.222300  \n",
       "59998 -0.071964  0.181192  -0.131681  \n",
       "59999 -0.048521  0.000586  -0.016812  \n",
       "60000 -0.365610  0.042036   0.036794  \n",
       "\n",
       "[60001 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = path.join(path_originales,\"train\")\n",
    "file = listdir(train)[1]\n",
    "arch = pd.read_csv(path.join(train, file))\n",
    "#arch.describe()\n",
    "s_df = estandarizar(arch)\n",
    "s_df.astype(\"float16\")\n",
    "#s_df.to_csv(path.join(path.join(path_copias, \"train\"), file), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comienzo con train\n",
      "Comienzo con test\n",
      "CPU times: user 56min 38s, sys: 32.9 s, total: 57min 11s\n",
      "Wall time: 57min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for carpeta in [\"train\", \"test\"]:\n",
    "    camino = path.join(path_originales, carpeta)\n",
    "    print(f\"Comienzo con {carpeta}\")\n",
    "    for file in listdir(camino):\n",
    "        lectura = path.join(camino, file)\n",
    "        escritura = path.join(path.join(path_copias, carpeta), file)\n",
    "        df = pd.read_csv(lectura)\n",
    "        s_df = estandarizar(df).astype(\"Float16\")\n",
    "        s_df.to_csv(escritura, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
