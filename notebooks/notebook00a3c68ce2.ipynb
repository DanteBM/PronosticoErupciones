{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "from threading import Thread\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "        #print(os.path.join(dirname, filename))\n",
    "#        pass\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/oscar/Escritorio/predict-volcanic-eruptions-ingv-oe/train\"\n",
    "train_filenames = os.listdir(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = train_filenames[0]\n",
    "data = pd.read_csv( os.path.join(train_dir,train_filename), dtype=\"Int16\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checando que tan buena estuvo la prank\n",
    "\n",
    "Según distintos archivos, hay ciertos que no tienen valores registrados (NaNs) o cosas raras como puros ceros. Sería importante ver como está dicha situación para todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prank(data):\n",
    "    \"\"\"Function to check if some columns contain only nans or some constant\"\"\"\n",
    "    nrows, _ = data.shape\n",
    "    only_nans = [data[sensor].isna().sum() for sensor in data.columns]\n",
    "    only_constants = [data[sensor].std() == 0 for sensor in data.columns]\n",
    "    \n",
    "    return only_nans, only_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_partition(data, npartitions):\n",
    "    n_elements = len(data)\n",
    "    n = n_elements//npartitions\n",
    "    \n",
    "    for k in range(npartitions):\n",
    "        i = k*n\n",
    "        j = (k+1)*n\n",
    "        if k != npartitions - 1:\n",
    "            yield data[i:j]\n",
    "        else:\n",
    "            yield data[i:]  # Last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(nthreads):\n",
    "    def decorator(original_function):\n",
    "        def parallelized_function(dir_path, filenames, verbose=True):\n",
    "            parallel_ans = []\n",
    "            threads = []\n",
    "            counter = 1\n",
    "    \n",
    "            # Partir los datos\n",
    "            for partition in yield_partition(filenames, nthreads):\n",
    "                thread = Thread(target=original_function, args=(dir_path, partition, verbose, parallel_ans))\n",
    "                threads.append(thread)\n",
    "                print(\"Starting\",counter)\n",
    "                thread.start()\n",
    "                counter += 1 \n",
    "            \n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "            \n",
    "            list_nans = [df_nan for df_nan, df_constant in parallel_ans]\n",
    "            list_constants = [df_constant for df_nan, df_constant in parallel_ans]\n",
    "            concatenation_nans = pd.concat(list_nans)\n",
    "            concatenation_constants = pd.concat(list_constants)\n",
    "            return concatenation_nans, concatenation_constants\n",
    "        \n",
    "        return parallelized_function\n",
    "    \n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@parallelize(8)\n",
    "def check_pranks(dir_path, filenames, verbose=True, parallel_ans = None):\n",
    "    nans = []\n",
    "    constants = []\n",
    "    for i,filename in enumerate(filenames,1):\n",
    "        df = pd.read_csv(os.path.join(dir_path,filename), dtype=\"Int16\")\n",
    "        only_nans, only_constants = check_prank(df)\n",
    "        nans.append(only_nans)\n",
    "        constants.append(only_constants)\n",
    "        if verbose and i%10 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    df_nans = pd.DataFrame(nans, columns=df.columns)\n",
    "    df_constants = pd.DataFrame(constants, columns=df.columns)\n",
    "    \n",
    "    if parallel_ans != None:\n",
    "        parallel_ans.append( (df_nans, df_constants) )\n",
    "    else:\n",
    "        return df_nans, df_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train_nans, df_train_constants = check_pranks(train_dir, train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_nans.to_csv(\"df_train_nans.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_nans.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_constants.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_constants.to_csv(\"df_train_constants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/home/oscar/Escritorio/predict-volcanic-eruptions-ingv-oe/test\"\n",
    "test_filenames = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Starting 4\n",
      "Starting 5\n",
      "Starting 6\n",
      "Starting 7\n",
      "Starting 8\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "20\n",
      "20\n",
      "20\n",
      "2020\n",
      "\n",
      "20\n",
      "20\n",
      "20\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "70\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "90\n",
      "90\n",
      "90\n",
      "9090\n",
      "\n",
      "90\n",
      "90\n",
      "90\n",
      "100\n",
      "100\n",
      "100\n",
      "100100\n",
      "\n",
      "100\n",
      "100\n",
      "100\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "110\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120120\n",
      "\n",
      "120\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "130\n",
      "140\n",
      "140\n",
      "140\n",
      "140\n",
      "140140\n",
      "\n",
      "140\n",
      "140\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "160160\n",
      "\n",
      "160\n",
      "160\n",
      "160\n",
      "160\n",
      "160\n",
      "160\n",
      "170\n",
      "170170\n",
      "\n",
      "170170\n",
      "170\n",
      "\n",
      "170\n",
      "170\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "180\n",
      "180180\n",
      "\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "190\n",
      "200200\n",
      "\n",
      "200\n",
      "200200\n",
      "\n",
      "200\n",
      "200200\n",
      "\n",
      "210\n",
      "210\n",
      "210\n",
      "210\n",
      "210210\n",
      "\n",
      "210\n",
      "210\n",
      "220\n",
      "220\n",
      "220\n",
      "220\n",
      "220220\n",
      "\n",
      "220\n",
      "220\n",
      "230\n",
      "230\n",
      "230\n",
      "230\n",
      "230\n",
      "230230\n",
      "\n",
      "230\n",
      "240\n",
      "240\n",
      "240\n",
      "240\n",
      "240240\n",
      "\n",
      "240\n",
      "240\n",
      "250250250\n",
      "\n",
      "\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "250\n",
      "260\n",
      "260\n",
      "260\n",
      "260260\n",
      "\n",
      "260260\n",
      "\n",
      "260\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "270\n",
      "280\n",
      "280\n",
      "280\n",
      "280\n",
      "280\n",
      "280\n",
      "280280\n",
      "\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "290\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "310\n",
      "320\n",
      "320320\n",
      "\n",
      "320\n",
      "320\n",
      "320\n",
      "320\n",
      "320\n",
      "330\n",
      "330\n",
      "330\n",
      "330\n",
      "330330\n",
      "\n",
      "330\n",
      "330\n",
      "340\n",
      "340\n",
      "340\n",
      "340\n",
      "340340340\n",
      "\n",
      "\n",
      "340\n",
      "350\n",
      "350350\n",
      "\n",
      "350350\n",
      "\n",
      "350350\n",
      "\n",
      "350\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "370\n",
      "380\n",
      "380\n",
      "380380\n",
      "\n",
      "380\n",
      "380\n",
      "380\n",
      "380\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "410410\n",
      "\n",
      "410\n",
      "410\n",
      "410\n",
      "410\n",
      "410\n",
      "410\n",
      "420\n",
      "420\n",
      "420\n",
      "420\n",
      "420\n",
      "420420\n",
      "420\n",
      "\n",
      "430\n",
      "430430\n",
      "\n",
      "430\n",
      "430\n",
      "430\n",
      "430\n",
      "430\n",
      "440\n",
      "440\n",
      "440440440\n",
      "\n",
      "\n",
      "440\n",
      "440440\n",
      "\n",
      "450\n",
      "450450\n",
      "\n",
      "450\n",
      "450\n",
      "450\n",
      "450450\n",
      "\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "460\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "470\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "480\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "490\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500500\n",
      "\n",
      "500\n",
      "510\n",
      "510\n",
      "510\n",
      "510510\n",
      "\n",
      "510\n",
      "510\n",
      "510\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "520\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "530\n",
      "540\n",
      "540\n",
      "540540\n",
      "\n",
      "540\n",
      "540\n",
      "540540\n",
      "\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "550\n",
      "560\n",
      "560\n",
      "560\n",
      "560560\n",
      "\n",
      "560\n",
      "560\n",
      "560\n",
      "CPU times: user 13min 47s, sys: 49.6 s, total: 14min 37s\n",
      "Wall time: 12min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test_nans, df_test_constants = check_pranks(test_dir, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nans.to_csv(\"df_test_nans.csv\", index=False)\n",
    "df_test_constants.to_csv(\"df_test_constants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor_1     23328531\n",
       "sensor_2     75807628\n",
       "sensor_3     26593834\n",
       "sensor_4        13406\n",
       "sensor_5     92651510\n",
       "sensor_6        57658\n",
       "sensor_7      2713162\n",
       "sensor_8     17653700\n",
       "sensor_9     51208744\n",
       "sensor_10    46050548\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_nans.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor_1       0\n",
       "sensor_2       0\n",
       "sensor_3       0\n",
       "sensor_4       0\n",
       "sensor_5       0\n",
       "sensor_6       0\n",
       "sensor_7       0\n",
       "sensor_8       0\n",
       "sensor_9       0\n",
       "sensor_10    815\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_constants.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# Zona después del \\end{document}\n",
    "\n",
    "`time_to_eruption` es el número de muestras antes de la siguiente erupción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
